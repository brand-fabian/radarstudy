# snakemake-dragen
The snakemake-dragen workflow was used for the processing of WGS sequencing data from `.fastq` or `.bam` to VCF with the help of `f1.4xlarge` instances in the Ireland region of AWS. The main rule of this pipeline is called `fastq2gvcf` and contains the invocation of the DRAGEN toolkit. 

Tested DRAGEN versions: `3.3.12`, `3.4.12`, `3.6.3`

Aside from invoking the small variant processing programs of the DRAGEN, this pipeline will also run DRAGENs implementation of the manta structural variant calling tool and perform the builtin CNV detection workflow. Keep in mind that in line with the goals of this study, each run is targeted for a trio of samples, and all possible DeNovo annotations of DRAGEN are included, even though they may not be used in later analyses.
## Usage
Example scripts for setting up an AWS node for running this snakemake pipeline and its execution are given in `setup.sh` and `run.sh`, respectively. Make sure to set the configuration values, especially `samplesheet` and `files` according to the location of these files. The `license_file` is read on the node executing snakemake and should only contain a valid DRAGEN license, it is subsequently passed down to DRAGEN invocations using a snakemake `param`. The `reference_dir` should contain an unpacked DRAGEN reference dictionary, in our case it was a DRAGEN converted GRCh37 reference. Please refer to the DRAGEN documentation to learn how to build a reference hash table for your DRAGEN and reference version combination.

Input files for this pipeline can be passed in two ways. The samplesheet contains two columns `fastq1` and `fastq2`, which can be used to pass exactly two fastq files for a single sample. This should be sufficient for most cases, but in case you have more `fastq` files for your samples, or want to start the analysis from a `.bam` file, the files table offers more flexibility to pass in a variable number of files for each sample. All files should reside in the input S3 bucket,  which must be accessible from all DRAGEN nodes without further authentication, as they will download the input files for each job once it starts. If any input file is missing in the input bucket pointed at by the `config.yaml` file, the pipeline will refuse to start, in order to make sure that no AWS compute time is wasted in futile attempts to start DRAGEN instances.
## AWS Environment
This pipeline is aiming to be executed within an AWS VPC environment. Therefore, all output files are uploaded to a configurable output bucket, while input files are taken from a different bucket. To avoid problems with snakemakes `--default-remote-provider` option, each processing rule has an accompanying `2remote` rule alongside it, which takes care of uploading all processed data to the output S3 bucket.

Due to missing AWS services capable of running the DRAGEN service through a scheduler compatible with snakemake at the time, we set up an autoscaling slurm cluster on AWS. The blueprint of the setup that we deployed for running this pipeline and processing 5k+ Genomes can be found [here](https://github.com/aws-samples/aws-plugin-for-slurm). We applied some changes to the script, to launch the correct instance types, mount shares and setup the DRAGEN system for processing but the general idea behind it remained.